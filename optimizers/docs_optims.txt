ВСЯ ПОДРОБНАЯ ДОКУМЕНТАЦИЯ В РЕПОЗИТОРИИ И/ИЛИ НА САЙТЕ ГДЕ-ТО... КОРОЧЕ НЕ ТУТ

SGD: Stochastic gradient descent optimizer. Оптимизатор стохастического градиентного спуска

Momentum: Оптимизатор с momentum, который помогает ускорить обучение и сгладить колебания

Adam: Адаптивный метод, который использует моменты первого и второго порядка для адаптации скорости обучения
Эффективен при работе с большими данными и высоко размерными пространствами.

RMSop: Оптимизатор, который использует скользящее среднее квадратов градиентов для адаптации скорости обучения.
Помогает предотвратить проблемы с исчезающим градиентом

NAG Optimization Modelling Suite — это комплексный набор надежных,
проверенных и документированных решений оптимизации для дискретной и непрерывной оптимизации, взят был из NAG.
(криво работает)

AdaDelta: этот оптимизатор является улучшенной версией Adam и требует меньшей настройки. Но Adam стабильнее
Использовать с осторожностью!
